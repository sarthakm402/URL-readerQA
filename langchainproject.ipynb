{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/Jul/2024 11:48:45] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2024 11:50:35] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention anything about the gold price rise, so I cannot answer the query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:51:01] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided content does not contain any information about gold prices, so I cannot answer the query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:51:49] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided content does not mention the amount of loans sanctioned by Indian fintech NBFCs, so I cannot answer this question from the provided content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:51:50] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided content does not mention the amount of loans sanctioned by Indian fintech NBFCs, so I cannot answer this question from the provided context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:52:12] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided content does not contain any information about loans or loan sanctions, so I cannot answer the query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:52:59] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$230 million\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Jul/2024 11:53:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2024 11:53:53] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$230 million\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from flask import Flask,request,render_template,jsonify\n",
    "urlss=[]\n",
    "app=Flask(__name__)\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('indexlangchain.html')\n",
    "@app.route('/get_answer',methods=['POST'])\n",
    "def gen():\n",
    " url=request.form.get('url')\n",
    " user_query=request.form.get('text')\n",
    " loader = UnstructuredURLLoader(urls=[url])\n",
    " urlss.append(url)\n",
    " data = loader.load()\n",
    " text_content = \"\\n\".join([doc.page_content for doc in data])\n",
    "\n",
    "\n",
    " try:\n",
    "     hf = ChatGoogleGenerativeAI(api_key=\" YOUR API KEY\",model=\"gemini-pro\")\n",
    " except Exception as e:\n",
    "     print(f\"Error initializing ChatGoogleGenerativeAI: {e}\")\n",
    "     raise\n",
    "\n",
    "\n",
    " # query = \"What is capital of India and UAE?\"\n",
    " # prompt = f\"\"\"\n",
    " # Answer the following question directly and accurately:\n",
    " # {query}\n",
    " # \"\"\"\n",
    " # print(text_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # try:\n",
    " #     result = hf.invoke(prompt)\n",
    " #     print(result)\n",
    " # except Exception as e:\n",
    " #     print(f\"Error invoking the model: {e}\")\n",
    " MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    " ]\n",
    "\n",
    " text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100,  \n",
    "    add_start_index=True,  \n",
    "    strip_whitespace=True, \n",
    "    separators=MARKDOWN_SEPARATORS, \n",
    " )\n",
    " text=text_splitter.split_text(text_content)\n",
    " template = \"\"\"Based on the following content, answer the query:\n",
    "            Content: \"{text}\"\n",
    "            Query: \"{query}\"\n",
    "            Answer:\"\"\"\n",
    " pr=PromptTemplate(input_variables=[\"text\",\"query\"],\n",
    "               template=template)\n",
    "\n",
    " llm_chain = LLMChain(llm=hf,prompt=pr)\n",
    "\n",
    " context = {\"text\":text, \"query\": user_query}\n",
    " response = llm_chain.invoke(context)\n",
    " answer = response[\"text\"]\n",
    " print(answer)\n",
    " return jsonify({\"answer\":answer})\n",
    "if __name__=='__main__':\n",
    "   app.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MORE REFINED CODE WITH CHECKS AND TRY EXCEPT BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/Jul/2024 11:19:34] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2024 11:19:50] \"POST /get_answer HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from flask import Flask, request, render_template, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('indexlangchain.html')\n",
    "\n",
    "@app.route('/get_answer', methods=['POST'])\n",
    "def gen():\n",
    "    url = request.form.get('url')\n",
    "    user_query = request.form.get('text')\n",
    "\n",
    "    # Load the URL content\n",
    "    loader = UnstructuredURLLoader(urls=[url])\n",
    "    data = loader.load()\n",
    "    text_content = \"\\n\".join([doc.page_content for doc in data])\n",
    "\n",
    "    try:\n",
    "        hf = ChatGoogleGenerativeAI(api_key=\"YOUR API KEY HERE\", model=\"gemini-pro\")\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Error initializing ChatGoogleGenerativeAI: {e}\"}), 500\n",
    "\n",
    "    # Text splitting\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n#{1,6} \",\n",
    "        \"```\\n\",\n",
    "        \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "        \"\\n---+\\n\",\n",
    "        \"\\n___+\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "\n",
    "    template = \"\"\"Based on the following content, answer the query:\n",
    "    Content: \"{text}\"\n",
    "    Query: \"{query}\"\n",
    "    Answer:\"\"\"\n",
    "    pr = PromptTemplate(input_variables=[\"text\", \"query\"], template=template)\n",
    "\n",
    "    llm_chain = LLMChain(llm=hf, prompt=pr)\n",
    "\n",
    "    context = {\"text\": \"\\n\".join(chunks), \"query\": user_query}\n",
    "    \n",
    "    try:\n",
    "        response = llm_chain.invoke(context)\n",
    "        answer = response[\"text\"]\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Error invoking the model: {e}\"}), 500\n",
    "\n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Munro, a Nobel prize winner and titan of the short story, died aged 92.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarthak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
